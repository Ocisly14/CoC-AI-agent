/**
 * CoC Agent Model Configuration
 * Default model settings for different providers and sizes
 */

import { ModelClass, ModelProviderName, Models } from "./types.js";

export const models: Models = {
  [ModelProviderName.OPENAI]: {
    endpoint: process.env.OPENAI_API_URL || "https://api.openai.com/v1",
    model: {
      [ModelClass.SMALL]: {
        name: process.env.SMALL_OPENAI_MODEL || "gpt-4o-mini",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 16384,
        frequency_penalty: 0.0,
        presence_penalty: 0.0,
        temperature: 0.6,
      },
      [ModelClass.MEDIUM]: {
        name: process.env.MEDIUM_OPENAI_MODEL || "gpt-4o",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 16384,
        frequency_penalty: 0.0,
        presence_penalty: 0.0,
        temperature: 0.6,
      },
      [ModelClass.LARGE]: {
        name: process.env.LARGE_OPENAI_MODEL || "gpt-4o",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 16384,
        frequency_penalty: 0.0,
        presence_penalty: 0.0,
        temperature: 0.6,
      },
      [ModelClass.EMBEDDING]: {
        name: process.env.EMBEDDING_OPENAI_MODEL || "text-embedding-3-small",
        dimensions: 1536,
      },
      [ModelClass.IMAGE]: {
        name: process.env.IMAGE_OPENAI_MODEL || "dall-e-3",
      },
    },
  },
  [ModelProviderName.ANTHROPIC]: {
    endpoint: process.env.ANTHROPIC_API_URL || "https://api.anthropic.com/v1",
    model: {
      [ModelClass.SMALL]: {
        name: process.env.SMALL_ANTHROPIC_MODEL || "claude-3-haiku-20240307",
        stop: [],
        maxInputTokens: 200000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.MEDIUM]: {
        name: process.env.MEDIUM_ANTHROPIC_MODEL || "claude-3-5-sonnet-20241022",
        stop: [],
        maxInputTokens: 200000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.LARGE]: {
        name: process.env.LARGE_ANTHROPIC_MODEL || "claude-3-opus-20240229",
        stop: [],
        maxInputTokens: 200000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
    },
  },
  [ModelProviderName.GOOGLE]: {
    endpoint: "https://generativelanguage.googleapis.com",
    model: {
      [ModelClass.SMALL]: {
        name: process.env.SMALL_GOOGLE_MODEL || "gemini-2.0-flash-exp",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.MEDIUM]: {
        name: process.env.MEDIUM_GOOGLE_MODEL || "gemini-2.0-flash-exp",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.LARGE]: {
        name: process.env.LARGE_GOOGLE_MODEL || "gemini-1.5-pro-latest",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.EMBEDDING]: {
        name: process.env.EMBEDDING_GOOGLE_MODEL || "text-embedding-004",
      },
    },
  },
  [ModelProviderName.GROQ]: {
    endpoint: "https://api.groq.com/openai/v1",
    model: {
      [ModelClass.SMALL]: {
        name: process.env.SMALL_GROQ_MODEL || "llama-3.1-8b-instant",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8000,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.MEDIUM]: {
        name: process.env.MEDIUM_GROQ_MODEL || "llama-3.3-70b-versatile",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8000,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.LARGE]: {
        name: process.env.LARGE_GROQ_MODEL || "llama-3.2-90b-vision-preview",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8000,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
    },
  },
  [ModelProviderName.OLLAMA]: {
    endpoint: process.env.OLLAMA_SERVER_URL || "http://localhost:11434",
    model: {
      [ModelClass.SMALL]: {
        name: process.env.SMALL_OLLAMA_MODEL || "llama3.2:3b",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.MEDIUM]: {
        name: process.env.MEDIUM_OLLAMA_MODEL || "hermes3:8b",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.LARGE]: {
        name: process.env.LARGE_OLLAMA_MODEL || "hermes3:70b",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.EMBEDDING]: {
        name: process.env.OLLAMA_EMBEDDING_MODEL || "mxbai-embed-large",
        dimensions: 1024,
      },
    },
  },
  [ModelProviderName.OPENROUTER]: {
    endpoint: "https://openrouter.ai/api/v1",
    model: {
      [ModelClass.SMALL]: {
        name: process.env.SMALL_OPENROUTER_MODEL || "nousresearch/hermes-3-llama-3.1-405b",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.MEDIUM]: {
        name: process.env.MEDIUM_OPENROUTER_MODEL || "nousresearch/hermes-3-llama-3.1-405b",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
      [ModelClass.LARGE]: {
        name: process.env.LARGE_OPENROUTER_MODEL || "nousresearch/hermes-3-llama-3.1-405b",
        stop: [],
        maxInputTokens: 128000,
        maxOutputTokens: 8192,
        frequency_penalty: 0.4,
        presence_penalty: 0.4,
        temperature: 0.7,
      },
    },
  },
};